---
title: "EDA"
author: "Liliana Millán, liliana.millan@gmail.com"
date: "September 2017"
output: 
  html_document:
    df_print: paged
    highlight: tango
    theme: lumen
---

![](../images/itam_logo.png)

## Agenda  {.tabset .tabset-fade .tabset-pills}

+ EDA
  + Tipos de EDA
  + Visualizaciones incorrectas
+ Gráficas
+ Formas deseables de datos
+ Casos de estudio

Estas notas estan basadas en el libro *Graphical Data Analysis with R*, la bibliografía se encuentra en el temario


### Exploratory Data Analysis

Es el PRIMER paso en el análisis de datos, es un punto CRÍTICO para realizar un análisis correcto -con contexto, sin sesgo, desde diferentes puntos de vista-

Puedes pensarlo como la primera aproximación al problema que quieres resolver

#### Objetivos 

+ Detectar de errores 
+ Detectar de datos anómalos, faltantes 
+ Detectar de datos aislados 
+ Verficar de que tienes datos relevantes y suficientes para contestar tu(s) pregunta(s)
+ Verificar si la(s) pregunta(s) es la correcta
   + ¿se requiren más datos?
+ ¿Qué otras variables puedes obtener de este set de datos que te permitan contestar tu(s) pregunta(s)? $\rightarrow$ lo veremos en feature extraction / feature selection
+ Verificar de suposiciones -tuyas-
+ Selección preliminar de los modelos apropiados 
+ Determinar relaciones entre las variables explicativas
+ Evaluar la dirección y tamaño -aproximado- de las relaciones entre las variables explicativas y la(s) variable(s) de salida -variable target-
+ Empezar a visualizar cómo sería la respuesta a la(s) pregunta(s) que quieres contestar con este set de datos
+ También nos ayudará cuando querramos automatizar la evaluación y tuneo de modelos

![](../images/pointer.png) Como verás ayuda a hacer un modelado más robusto

![](../images/pointer.png) De manera no estricta, si un análisis de datos no incluye modelado estadístico formal y/o inferencia/predicción entonces el análisis es EDA

#### Tipos de EDA 

Normalmente los datos que tenemos para analizar vienen en un formato rectangular (..aunque nunca limpios ni cómo los necesitamos (╯°□°)╯︵ ┻━┻, pero lo veremos más adelante) donde un renglón es una observación de un *experimento* y cada columna es: el identificador del sujeto, la variable de salida y las variables explicativas.

Analizar los datos de esta *tabla* resulta tedioso, aburrido y abrumador de entender, *enter EDA* :), las técnicas utilizadas en el EDA permiten esconder ciertas cosas de los datos para hacer sobresalir o dejar claras otras.

Hay 2 grandes maneras cruzadas de clasificar el tipo de EDA: 

1. Hacerlo de manera gŕafica o no -GEDA Graphical Exploratory Data Analysis, EDA Exploratory Data Analysis-
2. Cada método es univariado o multivariado -nonrmalmente bivariada-


+ **Análisis exploratorio no gráfico:** Generalmente incluye calcular el resúmen estadístico de 
+ **Análisis exploratorio gráfico:** Resúmen los datos de forma gráfica 
+ **Univariada:** Analizan una variable a la vez 
+ **Multivariada:** Analizan 2 o más variables a la vez para explorar relaciones entre variables -normalmente bivariada-

![](../images/pointer.png) Se recomienda primero hacer un EDA univariado a cada variable que forme parte de un EDA multivariado ANTES de hacer el EDA multivariado

Después de clasificar en estos 4 tipos cruzados existen más divisiones al EDA basadas en: 

a. El **rol de la variable**: salida o explicativa
b. El **tipo de la variable**: categórica o numérica 


#### Algunos consejos

+ Sí existen guías para qué técnica de EDA utilizar dependiendo de las circunstancias, pero normalmente eso se va mejorando con la experiencia
+ No existe un tipo óptimo de gráfica, prueba varias $\rightarrow$ aunque si hay reglas que debes seguir para la visualización
+ Antes de ponerte a hacer gráficas piensa qué quieres visualizar y por qué

#### Ejemplos de lo que NO debes hacer en visualizaciones

+ NO Graficar pies! ¿por qué? 

Aunque es una visualización muy solicitada por gente de negocio, existe una mejor manera de representar la misma información. El objetivo de una visualización de pie es mostrar cómo un 100% se distribuye entre diferentes valores de una variable. Una mejor manera de visualizar la misma información es haciendo una grfáica de barras horizontales ordenando de la proporción más grande a la más pequeña, el eje x tiene que ir de 0 a 100%.

```{r echo=F, warning=F, error=F, message=F}
library(ggplot2)
library(plotly)
library(knitr)
library(dplyr)


df <- data.frame(departamento=c("ventas","adquisiciones","rh","ti","finanzas","atencion_clientes","contabilidad"), 
                 prop=c(1,3,4,5,7,20,60))

df$departamento <- factor(df$departamento, levels=c("ventas","adquisiciones","rh","ti","finanzas","atencion_clientes","contabilidad"))


ggplot(df, aes( x="",y=prop, fill=departamento)) +
  geom_bar(width=1, stat="identity") +
  coord_polar("y", start=0) + 
  theme_bw() 


ggplot(df, aes(y=prop, x=departamento, fill=departamento)) +
  geom_bar(stat="identity") +
  coord_flip() +
  theme_bw() +
  scale_y_continuous(limits=c(0,100))


```


Si no te queda de otra más que presentar un pie, asegúrate que las proporciones suman 100%!, ordena las divisiones de mayor a menor...

¿Algo malo aquí?

![](../images/fake_data_visualizations_1.png)

\* Imagen tomada de [flowingdata.com](https://flowingdata.com/2009/11/26/fox-news-makes-the-best-pie-chart-ever/)

¿Qué? (╯°□°)╯︵ ┻━┻

![](../images/fake_data_visualizations_2.png)

\* Imagen tomada de [flowingdata.com](https://flowingdata.com/2010/05/14/wait-something-isnt-right-here/)

Paren!!!! (╯°□°)╯︵ ┻━┻

![](../images/fake_data_visualization_3.png)

\* Imagen tomada de [flowingdata.com](https://flowingdata.com/2012/11/09/incredibly-divided-nation-in-a-map/)

![](../images/pointer.png) Moraleja: Pon atención a tus visualizaciones son TAN IMPORTANTES como los modelos que realizas, es tu responsabilidad presentar información precisa, sin sesgo y que permita a los demás tomar decisiones basadas en ellas.

Como **NO** hacer una gráfica de barras 

![](../images/wrong_data_visualizations_1.png)

\* Imagen tomada de [viz.wtf](http://viz.wtf/)


No hagan esto!!!!  (╯°□°)╯︵ ┻━┻   $\rightarrow$ Por eso en esta clase están prohibidos los pie!!!

![](../images/wrong_data_visualization_2.png)


\* Imagen tomada de [viz.wtf](http://viz.wtf/)


### Tipos de variables

+ Datos Numéricos
    + Numéricas continuas: métricas, ...
    + Numéricos discretos: conteos, enteros
+ Datos categóricos
    + Categóricas nominales
    + Categóricas ordinales -aunque muchas veces pueden ser tomados como numéricos-
+ Texto, audio, imágenes

### EDA no gráfico

#### Univariado

##### Datos categóricos

¿Qué podemos encontrar? 

+ Valores faltantes
+ Proporciones
+ Frecuencias

$\rightarrow$ Generar una tabla de frecuencias de cada categoría. Por ejemplo:

```{r echo=F, warning=F, message=F, error=F}
df <- data.frame(category=c("girls","boys","women","man","total"),
                 count=c(2,4,3,6,15),
                 proportion=c(2/15,4/15,3/15,6/15, sum(2/15,4/15,3/15,6/15)))
                 

kable(df)
```


![](../images/pointer.png) Es muy importante obtener los totales para identificar errores, faltantes, anomalías en los datos

##### Datos numéricos

Quisieramos conocer algunas métricas de centralidad: modalidad (número de modas), mediana, media; dispersión: desviación estándar, forma -distribución teórica- para identificar si tiene colas pesadas: \*skewness -medida de asimetría- e identificación de outliers.

**Skewness:** valores cercanos a 0 indican muy poco skewness, si el número es negativo la cola es a la izquierda, si el número es positivo la cola es a la derecha 

![](../images/skewness.png)

<br>

La mayoría de estas métricas las puedes obtener con un `summary` en R. Por ejemplo: 

```{r echo=T, warning=F, message=F, error=F}
data(mtcars)

glimpse(mtcars)
summary(mtcars)
```

#### Multivariado

En el análisis multivariado lo que queremos encontrar son relaciones entre varias columnas -normalmente 2- 

##### Datos categóricos

+ Cuando queremos comparar 2 variables (con pocas categorías) se puede ocupar un **cross-tabulation** donde los valores de una variable se ponen en las columans y los valores de la otra variable en renglones para armar una matriz de frecuencias.

Por ejemplo: 

|Edad/Sexo|Mujer|Hombre|Total|
|:---:|:---|:----:|:---:|
|Jóven|2|3|**5**|
|Adulto|3|5|**8**|
|Adulto mayor|4|2|**6**|
|**Total**|**9**|**10**|**19**|

##### Datos numéricos

Lo más común es obtener la correlación y la covarianza entre 2 variables numéricas. Con la covarianza queremos ver qué tanto cambia una variable si la otra lo hace. 

Es más sencillo identificar estos cambios con la correlación, ya que esta va de [-1,1] donde -1 indica que las variables tiene una correlación lineal perfecta negativa, 1 indica que las variables tiene una correlación lineal perfecta positiva y 0 que las variables no tienen correlación. De nuevo, estas métricas son más sencillas de analizar visualmente-.

Cuando se tienen más de 2 variables numéricas normalmente se realizan matrices de covarianzas y correlaciones.

### GEDA 

#### Datos Numéricos

¿Qué podríamos encontrar?

+ Simetría/Asimetría
+ Huecos
+ Outliers
+ Multimodalidad
+ Amontonamientos (heaping)
+ Redondeos
+ Imposibilidades
+ Errores

![](../images/pointer.png) ¿Cómo podríamos visualizar estas características? 

+ **Histograma**: Simetría/Asimetría, huecos, multimodalidad, aproximación a una distribución **empírica**
+ **Diagrama de caja y brazos** -boxplot-: outliers, amontonamientos
+ **Scatterplot**: Cada dato es un punto, permite identificar huecos
* **Rugplot**: Gráfica que agrega pequeñas líneas verticales (eje x) u horizontales (eje y), se ocupa como un extra a un scatterplot, pero solo se recomienda cuando se tienen pocos datos $\rightarrow$ veremos ejemplos de esta gráfica
+ **Density estimate**: Como un modelo de tus datos... solo ten cuidado con los límites de las variables
+ **Distribution estimate**: Permite comparar distribuciones como por ejemplo si una está adelante de otra
+ **QQ-plot**: Permite comparar dos distribuciones, la tuya vs una teórica (por default la normal)


#### Ejemplo

El siguiente histograma corresponde a las velocidades del camponato mundial de ski del 2011, ¿qué puedes decir de la gráfica?

![](../images/ski_histogram.png)

<br>

Con el siguiente histograma ¿cambia en algo la historia/conclusión/opinión que tenías con el histograma anterior? 

![](../images/ski_gender_histogram.png)

<br>

![](../images/pointer.png) es por esto que debes hacer un análisis exploratorio profundo, ver todos los puntos de vista posibles -diferentes gráficas, diferentes análisis-

#### Otro ejemplo

¿Qué puedes decir de esta gráfica? 

![](../images/berkeley_admissions.png)

<br>

¿Y ahora? 

![](../images/berkeley_admission_gender.png)

#### Otro ejemplo 

Utilizaremos el set de datos de precios de vivienda de Boston que viene en el paquete `MASS`, este set de datos contiene 14 variables y 506 observaciones de áreas alrededor de la ciudad de Boston. 

+ crim: per capita crime rate by town
+ zn: proportion of residential land zoned for lots over 25,000 sq ft
+ indus: proportion of non-retail business acres per town
+ chas: Charles River dummy variable (=1 if tract  bound river; 0 otherwise)
+ nox: nitrogen oxiides concentration (parts per 10 million)
+ rm: average number of rooms per dwelling
+ age: proportion of onwer-occupied units built prior to 1940
+ dis: weighted mean of distances to fice Boston employment centres
+ rad: index of accessibility to radial highways
+ tax: full-value propoerty-tax rate per \\$10,000
+ ptratio: pupil-teacher ratio by town
+ black: proportion of blacks by town  ????
+ lstat: lower status of the population (percent)
+ medv: median value of owner-occupied homes in \\$1000

```{r echo=F, warning=F, message=F, error=F}
library(MASS)
library(dplyr)
library(tidyr)
library(ggplot2)
library(readODS)

data("Boston")

head(Boston)
```

En particular nos interesa revisar la varaible **medv** 

+ Empecemos sin visualización, generemos una tabla con los diferentes valores de *medv* para ver qué información podemos encontrar

```{r echo=T, warning=F, message=F, error=F}
table(Boston$medv)
```

Espero que no pienses diferente... pero tratar de sacar información de esta tabla esta difícil, solo podemos ver rápidamente que todos los números están redondeados a 1 decimal, fuera de eso... está complicado. Mejor visualizemos...

```{r echo=T}
ggplot(Boston, aes(x=medv)) + 
  geom_histogram() + 
  theme_bw() + 
  ggtitle("Valor medio de las casas (en miles de dólares")
```

![](../images/pointer.png) Además del warning que regresa R al generar el histograma ¿qué información puedes extraer?

+ ¿Qué habrá pasado cuándo el valor *medv* anda en 25? Se ve una caída interesante
+ El valor de *medv* parece inusual ...

¿Qué pasa si cambiamos el tamaño de los bines? 

```{r echo=T, warning=F, error=F, message=F}
ggplot(Boston, aes(x=medv)) + 
  geom_histogram(binwidth=1) + 
  theme_bw() + 
  ggtitle("Valor medio de las casas (en miles de dólares")
```

Hay detalles que antes no se percibían, como que en 34 hay una caída que no se había identificado antes

Ahora veamos todas las variables:

```{r echo=T}
B2 <- gather(Boston, bos_vars, bos_values, crim:medv)
ggplot(B2, aes(bos_values)) +
    geom_histogram() + 
    theme_bw() + 
    facet_wrap(~ bos_vars , scales = "free")
```

Este es un buen punto de partida, pero el tamaño de los bines no beneficia a todas las variables, cada variable debe tener su tamaño de bin adecuado, por otro lado la escala varía mucho entre variables: hay unas que van de 0 a 20 y otras de 0 a 400.

#### Ejercicio

1. Utiliza la variable *medv* para generar las siguientes gráficas en R: ¿Qué observas en cada una de estas gráficas?

+ `?boxplot`
+ `?stripchart` Un scatterplot de 1 dimensión, puede ser una alternativa al boxplot cuando tienes **pocos** datos
+ `?stem`
+ `?density` Necesitarás agregar un plot para ver la gráfica
+ `?rug` Primero necesitarás generar o el stripchart o el density para agregarle rug



```{r echo=T, warning=F, message=F, error=F}
#boxplot
ggplot(Boston, aes(x="var", y=medv)) +
  geom_boxplot() +
  theme_bw() + 
  coord_flip() +
  ggtitle("boxplot variable medv")

#stripchart
stripchart(Boston$medv)

#stem
stem(Boston$medv)

#density
plot(density(Boston$medv))

#rug
rug(Boston$medv)
```

2. En la gráfica de las 14 variables mostrada arriba ¿Cómo describirías las distribuciones? ¿Para cuales variables sería mejor utilizar boxplot? ¿Por qué?

```{r echo=T, warning=F, error=F, message=F}
ggplot(Boston, aes(x="var", y=black)) +
  geom_boxplot() +
  theme_bw() +
  coord_flip() + 
  ggtitle("Boxplot variable black")

ggplot(Boston, aes(x="var", y=crim)) +
  geom_boxplot() +
  theme_bw() +
  coord_flip() + 
  ggtitle("Boxplot variable crim")

ggplot(Boston, aes(x="var", y=tax)) +
  geom_boxplot() +
  theme_bw() +
  coord_flip() + 
  ggtitle("Boxplot variable tax")

ggplot(Boston, aes(x="var", y=zn)) +
  geom_boxplot() +
  theme_bw() +
  coord_flip() + 
  ggtitle("Boxplot variable zn")
```

#### Otro ejemplo 

Veamos ahora un poco de outliers con un dataset de películas que viene en el paquete `ggplot2movies` -antes formaba parte del paquete ggplot pero se hizo su propio paquete para bajar el overhead de bajar ggplot2- 

```{r echo=T, warning=T, error=F, message=F}
library(ggplot2movies)
library(dplyr)
library(scales)
library(plotly)

data(movies)

glimpse(movies) 

## histograma
ggplot(movies, aes(x=length)) + 
  geom_histogram() +
  scale_y_continuous(label=comma) + 
  scale_x_continuous(label=comma) + 
  theme_bw()

```

¯\\(º_o)/¯ chanclas... no dice mucho la gráfica... ¿o si? y además, ¿por qué sale más de 4,000 en longitud de la película? veamos qué hay ahí

```{r echo=T, warning=F, error=F, message=F}
movies %>% 
filter(length > 2000) %>% 
arrange(desc(length))
```

Para encontrar estos *outliers* es más sencillo hacer diagramas de caja y brazos

```{r echo=T, warning=F, error=F, message=F}
p <-ggplot(movies, aes(x="var", y=length)) +
  geom_boxplot() +
  scale_x_discrete(breaks=NULL) +
  scale_y_continuous(label=comma) +
  theme_bw() + 
  coord_flip()

p 

#o con plotly para saber los numeros :P
ggplotly(p)
```

Puedes obtener los outilers (valores) con la función boxplot 

```{r echo=T, warning=F, error=F, message=F}
outliers_info <- boxplot(movies$length)
summary(outliers_info) # las cosas que te devuelve el outlier 

# cuantos 
format(outliers_info$out %>% length(), big.mark=",")
```

Veamos cómo se ve el histograma si quitamos estos valores atípicos

```{r echo=T, warning=F, error=F, message=F}
ggplot(movies, aes(x=length)) + 
    xlim(0, 180) +
    geom_histogram(binwidth = 1) +
    xlab("Duración de películas en minutos") + 
    theme_bw()
```

que diferencia!

![](../images/pointer.png) **Ejercicio 1/Tarea 3** A entregar el **lunes 9 de octubre 2017** en tu carpeta dentro de carpeta `alumno` con el nombre ejercicio_eda_1 (entregar Rmd y html) 

a. ¿Qué puedes decir de esta gráfica?
b. ¿Cómo la modificas para agregar más ticks?
c. Haz una gráfica que muestre que los picos de 7 y 90 minutos existían antes y después de 1980
d. Existe la varaible `short` que indica si una película es "corta", ¿Qué gráfica puedes hacer para identificar el criterio que se ocupó para definir esta variable y cuáles están mal clasificadas? 

*** 

#### Datos categóricos

¿Qué podemos encontrar?

+ Patrones insesperados en los resultados
+ Malas Distribuciones
+ Categorías extras
+ Experimentos no balanceados
+ Muchas categorías
+ "No sé", Errores, Faltantes

![](../images/pointer.png) ¿Cómo podríamos visualizar estas características? 

+ **Barchars:** Nominales, Variables discretas
+ Explora (o ten cuidado con) diferentes ordenamientos
+ Es posible que datos ordinales entren también en este análisis

#### Bivariado contínuo

¿Qué podemos encontrar?

+ Relaciones lineales y no lineales
+ Asociaciones
+ Outliers: Se puede ser outlier en una dimensión y no serlo en dos, o viceversa
+ Clusters
+ Huecos
+ Barreras
+ Relación condicional

Modelos y pruebas

+ Correlación
+ Smoothing
+ Regresión lineal $\rightarrow$ Verifica los supuestos!!!! 

¿Regresión lineal? ... residuales 

![](../images/oreally.png)


#### Ejemplos

Ahora veamos la varaible `rating` que representa el promedio de calificaciones de IMDB y la variable `votes` que representa el número de personas que calificaron la película

```{r echo=T, warning=F, message=F, error=F}
ggplot(movies, aes(x=votes, y=rating)) +
  geom_point() +
  ylim(1,10) + 
  scale_x_continuous(label=comma) + 
  theme_bw()
```

![](../images/pointer.png) **Ejercicio 2/Tarea 3** A entregar el **lunes 9 de octubre 2017** en tu carpeta dentro de carpeta `alumno` en el mismo archivo creado en el ejercicio 1

a. Agrega `alpha-blending` ¿Qué pasa con los outliers? ¿Diferentes valores funcionan mejor?
b. ¿Cómo se ve la gráfica si remueves las películas con menos de 100 votos?
c. ¿Cómo se ve la gráfica si remueves todas las películas que tienen un rating arriba de 9?

***

Es posible estudiar posibles modelos (al igual que en el caso univariado) por ejemplo, ocupando el set de datos `Cars93` en el paquete MASS:

```{r echo=T, warning=F, message=F, error=F}
data("Cars93")

ggplot(Cars93, aes(x=Weight, y=MPG.city)) +
  geom_smooth(colour="green") +
  ylim(0, 50) + 
  scale_x_continuous(label=comma) +
  geom_point() +
  theme_bw() 
```

![](../images/pointer.png) **Ejercicio 3/Tarea 3** 

a. ¿Cuál es el outlier de la izquierda?
b. En muchos países en lugar de medirse en millas por galón, se mide en litros por 100 km. ¿Qué pasa si graficas `MPG.city` contra `Horsepower`? ¿Existe una relación lineal? ¿Cuáles son los outliers?

*** 

También se podemos hacer una matriz de scatterplots -**splom** (como lo hicimos con los histogramas :)), para ello ocupamos el método `ggpairs` de la librería `GGally` en el dataset de precios de vivienda ade Boston.

```{r echo=T, message=F, error=F, warning=F}
library(GGally)


dplyr::select(Boston, -rad, -chas) %>% 
ggpairs(title="Boston dataset", diag=list(continuous="density", axisLabels='none'))
  
```

![](../images/pointer.png) **Ejercicio 4/Tarea 3** 

a. ¿Cuáles están positivamente correlacionados con `medv`?
b. La variable `crim` -tasa de crímenes per cápita- tiene *scatterplots* con forma inusual, donde los valores altos de `crim` solo ocurren para un valor de la otra variable ¿Qué explicación le puedes dar?
c. Hay varias formas en los *scatterplots*, escoge 5 y explica cómo las interpretas

*** 

#### Más de 2 variables continuas

Para ver variables variables continuas se puede ocupar el *parallel coordinate plot* **pcp**, estos diagramas permiten dar un vista rápida a las distribuciones univariadas de varias variables a la vez: si son skew, si hay outliers, si hay gaps, etc. 

Ahora utilizaremos el dataset `iris` para probar estas gráficas utilziando de la librería `GGally` el método `ggparcoord`

```{r echo=T, warning=F, error=F, message=F}
data(iris)

ggparcoord(iris, columns=1:4, groupColumn = "Species") 
```

Esta está más difícil de interpretar... 

En un **pcp** cada línea es una observación del dataset, y cada atributo/variable del set es un punto en la gráfica. 

Lo que uno observa en la gráfica depende del orden en el cuál se dibujan los ejes. Hay autores que sugieren intentar varias combinaciones o inclusive poner varias copias de los ejes. Quizá lo mejor sea tener una gráfica interactiva para tal efecto... lo veremos más adelante. 

Este tipo de gráficas se puede usar para comparar modelos, series de tiempo, análisis de clusters, índices, etc. Esto lo discutiremos más adelante en el curso. Lo que se busca al ocupar esta gráfica es encontrar **similitudes** al comparar diferentes características del dataset.

Hay varias cosas que ajustar en estas gráficas para poder ser interpretadas: el orden de las variables y el escalamiento de los datos. 

+ **Escalamiento:** Cuando vamos a comparar diferenes mediciones tenemos que determinar alguna manera de ponerlos en la misma escala. En las gráficas **pcp** por default, la escala depende del máximo y mínimo valor encontrado por cada variable, el mínimo a $0$ y el máximo se maperará a $1$ (o 0% y 100%) 

$$y_{ij}=\frac{x_{ij} - \min_i x_{ij}}{\max_i x_{ij} - \min_i x_{ij}}$$

Otra forma de escalar los datos es usando la desviación estándar o el rango intercuantil (IQR), por ejemplo: 

$$z_{ij}=\frac{x_{ij} - \bar{x}_j}{sd(x_j)}$$

Este es el escalamiento por default que ocupa `ggparcoord`.

![](../images/pointer.png) La escala de cada variable es propia a esa variable, es decir, no se deben comparar las alturas entre diferentes variables (diferentes puntos en el eje x)

```{r echo=T, warning=F, message=F, error=F}
iris1 <- iris
names(iris1) <- c(abbreviate(names(iris)[1:4]), "Species")
a1 <- ggparcoord(iris1, columns = 1:4, 
                 alphaLines = 0.7,  
                 groupColumn = "Species") + 
  ggtitle("a1")
a2 <- ggparcoord(iris1, columns = 1:4, 
                 scale="uniminmax", 
                 alphaLines=0.7, 
                 groupColumn = "Species") + 
  ggtitle("a2")
a3 <- ggparcoord(iris1, columns = 1:4, 
                 scale="globalminmax", 
                 alphaLines=0.7, 
                 groupColumn = "Species") + 
  ggtitle("a3")
a4 <- ggparcoord(iris1, columns = 1:4, 
                 scale="center", 
                 scaleSummary="median", 
                 alphaLines=0.7, 
                 groupColumn = "Species") +
  ggtitle("a4")

gridExtra::grid.arrange(a1, a2, a3, a4)
```

![](../images/pointer.png) **Ejercicio 5/Tarea 3** 

a. Usando el dataset `Boston` realiza un *pcp*, intenta resaltar las características que haz observado en los ejercicios anteriores. Piensa cómo le hiciste...

*** 

#### Varias variables categóricas

Cuando queremos comparar varias variables categóricas al mismo tiempo tenemos el problema de que haya muchas categorías por variable y la gran cantidad de posibles ordenamientoso de las variables. Por ejemplo, para $J$ variables categóricas con $c$ número de categorías, las variables pueden ser ordenadas de $J!$ maneras diferentes y las categorías dentro de las variables de $\Pi_{j=1}^J c_j!$, lo cual da un total de $J!\Pi_{j=1}^Jc_j!$ ordenamientos, o sea un montón...

Los tipos de gráficas que podemos ocupar en estas situaciones son: 

+ *mosaicplots*
+ *doubledecker plots*
+ *fluctuation diagrams*
+ *treemaps*
+ *association plots*
+ *parallel sets/categorical parallel coordinate plots*

![](../images/pointer.png) [New approaches in Visualization of Categorical Data: R Package extracat de A. Pilhöfer y A. Unwin. JSS, Vol53 issue 7, May 2013.](https://www.jstatsoft.org/article/view/v053i07/v53i07.pdf)

##### Doubledecker 

En esta gráfica las variables se dividen en variables explicativas y variable objetivo, esta última ocupa el eje vertical 

```{r echo=T, warning=F, message=F, error=F}
library(vcd)

data(Titanic)

#variables a ocupar para hacer una tabla de contingencia, la variable tardet se pone al final
doubledecker(Survived ~ Sex, data=Titanic, gp=gpar(fill=c("grey90", "darkblue")))

doubledecker(Survived ~ Class, data=Titanic, gp=gpar(fill=c("grey90", "darkblue")))
```

<br>

Conforme más variables agregamos se vuelve más difícil la interpretación: 

```{r echo=T, warning=F, message=F, error=F}
doubledecker(Survived ~ Sex + Class, data=Titanic, gp=gpar(fill=c("grey90", "darkblue")))
```

<br>

Además, como mencionábamos, al ser una tabla de tipo *mosaic* la construcción de la gráfica depende del orden de las variables

```{r echo=T, warning=F, message=F, error=F}
doubledecker(Survived ~ Class + Sex, data=Titanic, gp=gpar(fill=c("grey90", "darkblue")))
```

<br>

##### Mosaicplot

En este tipo de gráfica ocupamos rectángulos proporcionales a los conteos de las combinaciones que los rectángulos representan. Se dibujan partiendo con un rectángulo que representa todo el *dataset*, luego se toma la primer variable y se parte el eje horizontal en secciones proporcionales a los tamaños de las categorías.

```{r echo=T, warning=F, message=F, error=F}
titanic <- as.data.frame(Titanic)
par(mfrow=c(2,2),  mar= c(4, 4, 0.1, 0.1))
mosaicplot(xtabs(Freq ~ Survived, data=titanic), main="")
mosaicplot(xtabs(Freq ~ Survived + Sex, data=titanic), main="")
mosaicplot(xtabs(Freq ~ Survived + Sex + Class, data=titanic), main="")
mosaicplot(xtabs(Freq ~ Survived + Sex + Class + Age, data=titanic), main="")
```

De nuevo el orden de las variables determinará la apariencia (y tal vez el *insight* que puedes obtener)

Otra opción es utilizar `pairs`

```{r echo=T, warning=F, message=F, error=F}
pairs(xtabs(Freq ~ ., data=titanic))
```

La diagonal de esta gráfica contiene gráficas de barra para las variables individuales y *mosaicplots* en los elementos que no están en la diagonal. Si hay muchas variables esta gráfica será muy difícil de leer.

Si existe una variable a explicar obvia (en nuestro caso Survived), quizá sea mejor tomar esto en cuenta en nuestro análisis:

```{r echo=T, warning=F, message=F, error=F}
ggplot(titanic, aes(Survived, Freq, fill=Sex)) + 
    geom_bar(stat = "identity") +
    theme_bw() +
    facet_grid(Class ~ Sex + Age) + theme(legend.position="none")
```


##### Fluctuation diagrams

Si queremos observar tablas de contingencia muy grandes o matrices de confusión, podemos usar las gráficas de tipo *fluctile*. En particular estas gráficas resaltan qué subgrupos aparecen más frecuentemente, o cuales combinaciones no aparecen en lo absoluto.

```{r echo=T, warning=F, error=F, message=F}
library(extracat)

#nota que requiere una tabla de entrada! no un data frame
fluctile(Titanic)
```

Por otro lado, si solo nos interesa comparar las tasas, podemos usar la funcion `rmb` -*relative multiple barchar*- con el parámetro `freq.trans="const"`:

```{r echo=T, warning=F, message=F, error=F}
rmb(formula = ~ Sex + Class + Age + Survived, 
              data=titanic, 
              cat.ord=2, spine=TRUE, freq.trans="const")
```

En esta gráfica vemos las tasas de supervivencia (el color verde) por subgrupo. Esto sería muy difícil de apreciar con un *mosaicplot* normal.

Las **rmb** mezclan las gráficas de barras y los *mosaicplots*.



### Forma deseable de los datos

Para muchos de los análisis de datos requerimos que los datos estén en formato *tidy*

+ ![](../images/pointer.png) [Artículo Tidy Hadley Wickham](http://vita.had.co.nz/papers/tidy-data.pdf)
+ También habrá ejemplos en los que no queremos que esté en formato *tidy* [Non Tidy Data](https://simplystatistics.org/2016/02/17/non-tidy-data/) + O grafos! (lo veremos más adelante)

![Tidy Data](http://r4ds.had.co.nz/images/tidy-1.png)

*Fuente: R for Data Science, Wickham and Grolemund, 2016*

Es decir:

1. Cada *variable* una columna
2. Cada *observación* un renglón
3. Cada *valor* una celda

![](../images/data_analysis_and_cleaning.png)

*Fuente: [Presentaciones de Hadley Wickham](http://vita.had.co.nz/papers/tidy-data-pres.pdf)*

#### Formatos salvajes

Los siguientes ejemplos de dataset son típicos de algunas fuentes de datos, por ejemplo: INEGI (al menos en formatos al 2014)

||Lat|Long|Indicador|
|:----:|:------:|:-------:|:--------:|
|Obs1| \# | \# | \# | 


Table: Fecha implícita

| | lugar | indicador |
|:---:|:-----:|:-----:|
|obs 1 |  | |
|obs 2 | |  |


Table: Otro con Fecha implícita 

| |Fecha 1 | Fecha 2 |
|:---:|:-----:|:------:|
|lugar 1|    |   |
|lugar 2|  | |

Table: Implícita la variable, ¿¿por qué?? (╯°□°)╯︵ ┻━┻

|  |  Fecha 1 | Fecha 2 | ... |
|:-----:|:-------:|:-------:|:------:|
| **LUGAR 1** |   |    |   |  |
| Ind 1 |   #  | #   | ... |
| Ind 2 |   #  |  #   | ... |
| **LUGAR 2** |   |    |   |  |
| Ind 1 |   #  | #   | ... |
| Ind 2 |   #  |  #   | ... |

O cosas más locas! 

||Indicador 1|Indicador 2|
|:--:|:--:|:---:|

||Fecha 1| Fecha 2| Fecha 1| Fecha 2|
|:-----:|:--:|:-------:|:-----:|:-----:|
|lugar 1| Ind 1| Ind 1 | Ind 2| Ind 2|
|lugar 1| Ind 1| Ind 1 | Ind 2| Ind 2|

Otro ejemplos: 

+ Nombres de las columnas representan valores de los datos en lugar de nombres de variables -el nombre de un lugar por ejemplo-
+ Una columna contiene varias variables en lugar de una variable 
+ Una tabla contiene más de una unidad de observación
+ Las variables están contenidas en los renglones y columnas, en lugar de sólo columnas.
+ Los datos de una unidad observacional están dispersas en varios *data sets*

Ejemplos:

```{r echo=TRUE, warning=FALSE, error=FALSE, message=FALSE}
library(tidyr)

messy <- data.frame(nombre=c("juan.perez.lopez","martha.lopez.benitez",
                             "jesus.ramirez.perez","jose.martinez.lopez",
                             "aurora.saldivar.salazar"),
                    genero_edad=c("m.35","f.23","m.30","m.25","f.33"),
                    time=c(1,3,4,5,6))

messy
```

Tendríamos que dejarlo *tidy*:

+ Separando el nombre
```{r echo=T, warning=F, error=F, message=F}
semi_messy <- messy %>% separate(col=nombre, into=c("nombre",
                                      "apellido_paterno",
                                      "apellido_materno"), 
                   sep="\\.")

semi_messy
```

+ Separando género y edad


```{r echo=T, warning=F, error=F, message=F}
clean <- semi_messy %>% separate(col=genero_edad, into=c("genero","edad"), 
                   sep="\\.")

clean
```

Otro ejemplo: ¿Qué está mal?

```{r echo=T, warning=F, error=F, message=F}
messy <- data.frame(pais=c(rep("Afganistan",4),
                           rep("Brazil",4),
                           rep("China",4)),
                    year=c(rep(1999,2),rep(2000,2),
                           rep(1999,2),rep(2000,2),
                           rep(1999,2),rep(2000,2)),
                    llave=c("casos","poblacion","casos","poblacion",
                          "casos","poblacion","casos","poblacion",
                          "casos","poblacion","casos","poblacion"),
                    valor=c(75,1300000,134,1400000,
                            10000,100000000,12000,120000000,
                            56000,150000000,60000,170000000))

messy
```

Lo deberíamos arreglar con: 

```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
clean <- messy %>% spread(key=llave, value=valor, fill=NA)

clean
```

El último

```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
stocks <- data_frame(
  time = as.Date('2009-01-01') + 0:9,
  X = rnorm(10, 0, 1),
  Y = rnorm(10, 0, 2),
  Z = rnorm(10, 0, 4)
)

stocks

stocks %>% gather(stock, price, -time)
```

### Casos de estudio

#### Reproducibilidad

En estos casos de estudio nos vamos a encontrar con nuestro primer tipo de *pipeline*, en este caso en particular, este *pipeline* no es para ejecutar grandes volúmenes de datos o para ejecutar contínuamente, sino para poder reproducir el proceso de exploración y modelado de datos.

![](../images/pointer.png) Antes de empezar te recomiendo ampliamente que utilices `packrat` para la administración de paquetes en R [packrat](https://rstudio.github.io/packrat/)

En tu carpeta crea las carpetas `german` y `algas` dentro de ellas crea los archivos: 

+ 00-load.R
+ 01-prepare.R
+ 02-clean.R
+ run.R

En estos archivos pondrás código para ejecutar los pipelines de los siguientes casos de estudio

### German

#### **¿Quién eres?**

Eres el científico de datos de un banco alemán, el banco tiene muchas pérdidas debido a malos créditos y quiere reducirlas. 
Te piden realizar esta tarea, indicando que quieren reducir la tasa de pérdidas en un 10%.

Neceistaras lo siguiente:

```{r eval=F}
rm(list = ls())

instalar <- function(paquete) {

    if (!require(paquete,character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)) {
        install.packages(as.character(paquete), dependecies = TRUE, repos = "http://cran.us.r-project.org")
        library(paquete, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)
    }
}

paquetes <- c('lubridate', 'magrittr', 'ggvis', 'dplyr', 'tidyr', 'readr', 'rvest', 
              'ggplot2', 'stringr', 'ggthemes', 'googleVis', 'shiny', 'tibble', 'vcd', 'vcdExtra',
              'GGally', 'readODS', 'readxl', "RSQLite")

lapply(paquetes, instalar);

source("metadata.R")
source("utils.R")
```


#### **Datos**  

Usaremos para este ejemplo, los datos de crédito alemán (*German data set*). 
[German Credit Data](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29)

#### **Carga de datos**

```{r echo=F, warning=F, message=F, error=F}
library(readr)
library(ggplot2)
library(tidyr)
library(dplyr)
library(stringr)
library(ggthemes)
```

```{r echo=T, warning=F, message=F, error=F}
german_url <- paste0('http://archive.ics.uci.edu/ml',
                    '/machine-learning-databases/statlog',
                    '/german/german.data')
german_data <- read_delim(german_url, 
                          col_names=F,
                          delim=" ")
```

Tenemos `r format(dim(german_data)[1], big.mark=",")` observaciones y  `r format(dim(german_data)[2], big.mark=",")` variables. Veamos cómo están los datos: 

```{r echo=T, warning=F, message=F, error=F}
head(german_data)
```

¿Qué? (╯°□°)╯︵ ┻━┻

**Ejercicio** 

+ Crea una función `load` en `utils.R` en tu carpeta que descargue **si y solo si** no existe un archivo `german.rds`, si no existe descarga y guarda el archivo. 
+ `?saveRDS`, `?readRDS`, `?file.exists`

#### **Transformación de datos**

Los nombres de las columnas fueron copiados a mano desde [`german.doc`](https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.doc), los nombres se encuentran en el archivo `metadata.R`

```{r echo=T, warning=F, message=F, error=F}
source("utils.R")

german_colnames
```

La variable de salida la definimos como categórica (`factor` en `R`)
```{r echo=T, warning=F, message=F, error=F}
colnames(german_data) <- german_colnames

german_data$good_loan <- as.factor(
                          ifelse(
                            german_data$good_loan == 1, 
                            'GoodLoan', 
                            'BadLoan'
                            )
                          )
```

#### **Decodificar**

- Crea una función `german_decode` en un archivo `utils.R` dentro de tu carpeta, 
esta función debe de utilizar `german_codes` (en el archivo `metadata.R`) para   
decodificar los elementos  de todas las columnas (por ejemplo `A201` -> `yes`)

- Utiliza `dplyr` para decodificar todas las columnas de `german_data`

![](../images/pointer.png) TIP: verifica el uso de `left_join`, `rbind` y `cbind`

```{r echo=T, warning=F, message=F, error=F}
german_data  <- german_data %>% 
                     mutate_all(funs(german_decode))

german_data
```

#### **Datos manejables**

En este momento deberás de tener archivos `00-load.R`, `01-prepare.R`, `02-clean.R`,  `metadata.R` 
y un archivo `utils.R` dentro de `german`.  
Además deberías de tener un archivo `german.rds`.

**Ejercicio**

- ¿Hay algo raro con los datos de préstamo?

- ¿Cuál crees que debería ser la distribución del resultado del 
  préstamo `Good_Loan` respecto a `Credit history`?

- Grafícalo y comenta tus resultados.

- Si lo vas a hacer con `ggplot2` usa este 
[cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/12/ggplot2-cheatsheet-2.0.pdf) 
o estos [ejemplos](http://docs.ggplot2.org/current/)

<div style="background-color:#ffcf40">

**Tarea 4** Entrega **lunes 16 de octubre 23:59:59 CST** en tu carpeta `alumnos/german`

+ Dentro de tu carpeta german tu archivo `utils.R`, `00-load.R`, `01-prepare.R`, `02-clean.R` y `run.R`. $\rightarrow$ Si tuvo sentido para ti poner las funciones en util y nada más ... entonces solo sube tu `util.R` -te recomiendo que aunque hayas creado las funciones ahí de todas maneras generes el archivo 00 y 01 por claridad y sanidad mental tanto tuya como la de tu equipo :)- 
+ Tener la función load completa: bajar el *dataset* `german` si no existe y guardárla como rds, o si ya existe, cargarla con readRDS
+ Tener la función de german_decode: si prefieres ocupar tu propia implementación adelante -de hecho es preferible- cada quién resuelve un problema a su manera y es mejor que te quede bastante claro como solucionar este problema porque te enfrentarás a él MUCHAS veces
+ ¿Encontraste algo raro en los préstamos? ¿Qué? ¿Cómo lo encontraste?
+ ¿Cuál crees que debería ser la distribución del resultado del 
  préstamo `Good_Loan` respecto a `Credit history`? Grafícalo y comenta tus resultados $\rightarrow$ checa los tips de como gráficarlo con ggplot2 -te van a salir cosas *raras=feas (visualmente)*- pero no te estreses, las arreglaremos la siguiente clase ╭(◔ ◡ ◔)/

</div>

![](../images/reproducibility_pipeline_a.png)

![](../images/reproducibility_pipeline_b.png)

**Ejercicio**

- Fue terrible poder hacer la gráfica con `ggplot2` utilizando los nombres de
columnas que pusimos (`german_colnames`).

- Modifica el archivo donde tengas `german_colnames` (puede ser `utils.R` o `metadata.R`) y 
sustituye (usando quizá `stringr` o `grep`) los `' '` y `'/'` por `'_'` (ve la [guía de 
estilo](http://adv-r.had.co.nz/Style.html)) y pasa todo a minúsculas.

- Ejecuta todo de nuevo (¡la ventaja de ser reproducible!)

```{r echo=T, warning=F, message=F, error=F}
colnames(german_data) <- german_clean_colnames(german_colnames)

colnames(german_data)
```

#### **Intermedio**

+ Si la gráfica de barras te quedó desacomodada, este código ordena los `bar charts` (Adolfo de Unánue)

```{r echo=T, warning=F, message=F, error=F, fig.width=13}
german_data %>% 
    group_by(credit_history) %>% 
    dplyr::summarise(count = n()) %>% 
    arrange(desc(count)) %>% 
    ggplot(.) + 
        geom_bar(aes(x=reorder(credit_history, count), y = count), stat="identity", fill="gray") + 
        coord_flip() + 
        theme_hc() + 
        ylab('casos') + 
        xlab('Historial de crédito')
```

Yo me lo sé pasando a factores los nombres y ordenándo los niveles como necesites presentarlos -salida de una `arrange`-

```{r echo=T, warning=F, message=F, error=F}
plot_sorted <- german_data %>% group_by(credit_history) %>%
  summarise(count=n()) %>%
  arrange(count) #como haremos coord flip necesitamos ordenarlas de manera inversa a como queremos que aparezcan en el coord_flip

plot_sorted$credit_history <- factor(plot_sorted$credit_history,
                                     levels=plot_sorted$credit_history)

ggplot(plot_sorted, aes(x=credit_history, y=count), fill="gray") +
  geom_bar(stat="identity") +
  coord_flip() +
  theme_hc() +
  ylab("casos") +
  xlab("Historial de crédito")
```


#### **Sanidad de los datos**

+ El nombre de la columna no significa lo que tu crees que significa

+ El significado de la columna cambia con el paso del tiempo o la metodología para medir esa variable.

+ Mucha / muy poca resolución

+ Los valores `missing` no son realmente faltantes (`NAs`), si no que significan algo
    + Regularmente no documentado
    
+ Si es un `csv` de seguro a alguien ya le pareció chistoso ponerle comas dentro de los valores de las columnas 
    + Existe una historia parecida para los `tsv`, `psv`, etc.

#### `summary`

Un uso del  `summary()` es detectar problemas en los datos.

- ¿Valores faltantes?
    - ¿Hay una variable con muchos faltantes? ¿Por qué? ¿Es un error? ¿Significa algo?
    
- ¿Valores inválidos?
    - ¿Hay negativos donde no debería de haber? (Como en edad, ingreso, estatura)
    - ¿Texto en lugar de números?


- ¿Outliers?
    - Son aquellos valores que no crees que deberían de estar (En el ejemplo de edad 1400 años)

- ¿Rangos?
    - Es importante saber cuanto varía la variable.
    - Si es muy amplio, puede ser un problema para algunos algoritmos de modelado.
    - Si varía muy poco (o nada) no puede ser usado como predictor.
    
- ¿Unidades?
    - ¿El salario es mensual?¿Quincenal?¿Por hora?
    - ¿Los intervalos de tiempo están en segundos?¿Años?
    - ¿Las longitudes? ¿La moneda?


**Ejercicio**

Revisa `german_data` con `summary()`, reporta alguna anomalía.


```{r echo=T, warning=F, message=F, error=F}
summary(german_data)
```

**Ejercicio**

Asegurar que el *dataset* esté en forma *tidy*, si no lo está haz que esté en formato tidy (puede quedar en prepare/utils) Guarda esto en `german-tidy.rds`


*** 

### Algas

#### ¿Quién eres? 

Como el dinero no alcanza, tomas otro trabajo rápido para una ONG. Quieren predecir 
la concentración de algas en ríos de la región. Tomaron datos durante un año. 

Cada observación es el efecto de agregar varias muestras de agua recolectadas 
en el mismo río por un periodo de 3 meses en la misma estación del año.


#### Datos 

Los datos provienen de [Coil 1999 Competition 
Data](https://archive.ics.uci.edu/ml/datasets/Coil+1999+Competition+Data) sobre contaminación de ríos.
La explicación de los datos se puede ver [aquí](https://archive.ics.uci.edu/ml/machine-learning-databases/coil-mld/coil.data.html)

```{r}
algas_url <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/coil-mld/analysis.data'

algas <- read_csv(algas_url, 
                  col_names = algas_colnames,
                  na = 'XXXXXXX')
```
**Ejercicio**

- Repite los pasos realizados para `german.data` con `algas`
- Revisa con `summary()`, reporta alguna anomalía.


```{r}
summary(algas)
```

¿Por qué la columna `NO3` **no** es numérica?

```{r}
problems(algas)
```

El problema lo podemos observar, por ejemplo, en la observación `20`

```{r}
algas[20,]
```

Otra cosa interesante a notar, es que hay justo `r length(problems(algas))` casos de `a7` con `NA`s. 
Al parecer el error en la columna de `NO3` se está "comiendo" a la columna `a7`.

```{r}
algas[problems(algas)$row,]
```

La columna de `NO3` está capturada a 5 decimales. Parece lógica la suposición de 
dividir esa columna a los 5 decimales.

Podemos limpiar esta columna haciendo lo siguiente

```{r}
problematic_rows <- problems(algas)$row

algas[problematic_rows,] <- algas %>% 
    slice(problematic_rows) %>% 
    unite(col="all", -seq(1:6), sep = "/", remove=TRUE) %>%
    extract(all, into=c("NO3", "NH4", "resto"),
            regex="([0-9]*.[0-9]{5})([0-9]*.[0-9]*)/(.*)/NA", remove=TRUE) %>%
    separate(resto, into=names(algas)[9:18], sep="/", remove=TRUE)    

algas[19:20,]
```

¿Qué pasó aquí? 

```{r eval=F}
algas %>%
  slice(problematic_rows) %>% head()

algas %>% 
  slice(problematic_rows) %>%
  unite(col="all", -seq(1:6), sep="/", remove=T)

algas %>% 
  slice(problematic_rows) %>%
  unite(col="all", -seq(1:6), sep="/", remove=T) %>%
  extract(all, into=c("NO3", "NH4", "resto"),
            regex="([0-9]*.[0-9]{5})([0-9]*.[0-9]*)/(.*)/NA", remove=T)
```

```{r}
algas <- readr::type_convert(algas)

algas
```


```{r eval=F}
algas <- algas %>%
            mutate_all(funs(algas_clean))

algas
```


Revisando el resumen estadístico con  `summary`:

```{r}
summary(algas)
```

Los tipos de datos:

```{r}
glimpse(algas)
```


#### Usando gráficas

- El resumen estadístico quizá no cuente toda la historia.
- El siguiente paso es explorar mediante gráficas.
- Es un proceso iterativo.

##### Una sola variable 

- ¿Cuál es el pico? ¿Coincide con la media? ¿La mediana? ¿Existe?
- ¿Cuántos picos?
  - Si es `bimodal` o `multimodal` quizá haya varias poblaciones en lugar de una y será mejor modelar por separado.
- ¿Qué tan normal es? ¿El log-normal? 
    - Utiliza una gráfica `Q-Q`
- ¿Cuánto varía? ¿Está concentrada en un intervalo o una categoría?
- ¿Outliers? $\rightarrow$ Usa gráficas de `boxplot`.
- Da preferencia a los `density plots`, en esta gráfica es más importante la forma que los valores actuales del eje vertical.
- Si los datos están concentrados en un solo lado de la gráfica (`skewed`) y es no negativa es bueno representarla en `log10`.
- Una grafica de barras no da más información que `summary()`, aunque algunos las prefieren.
    - Es bueno mostrarla horizontal y ordenada.

##### Dos variables

- ¿Existe relación entre dos variables de entrada? ¿entre una entrada y la variable de salida?
- ¿Qué tan fuerte?
- ¿Qué tipo de relación?
- `Scatter plot` entre dos variables numéricas, calcular la correlación de Pearson en un conjunto `sano` de los datos, visualizar la curva que mejor representa los datos.
- `Stacked bar charts` para dos variables categóricas.
    - Si quieres comparar razones a lo largo de las categorías lo mejor es usar un `filled bar chart`. En este caso se recomienda agregar un  `rug` para tener una idea de la cantidad de individuos.
    - Si hay múltiples categorías por variable, es mejor usar `facets`.
- Para variable categórica y numérica es recomendable usar `boxplot` (en su versión de `violin` o `jitter`).


<div style="background-color:#ffcf40">

Tarea 5 **Ejercicio**. Rmd y html en el git dentro de tu carpeta con el nombre tarea_5_eda. Se entrega máximo el **23 de octubre 2017 23:59:59 CST** (-0.5 por cada día de retraso). Enjoy! ╭(◔ ◡ ◔)/  

- Es importante en la etapa de exploración, poder generar varias gráficas de manera automática 
  y simple para analizarlas visualmente y tener una idea de los datos. 
- Crea una función que genere los tipos de gráfica para cada par de variables del `data.frame` (en realidad es un `tibble`).
  Esta función debe de recibir dos parámetros, uno que indique si genera todas las combinaciones 
  de dos variables o recibe una lista de variables en las cuales generar las combinaciones.
- Guárdala en `utils.R`. 
- Crea en `03-eda.R` en ambas carpetas: `algas` y `german`.

</div>

#### Valores faltantes: `NAs`

- Los pasos son los siguientes:
    - Identificar los datos faltantes.
    - Examinar las causas de los datos faltantes. 
        - Preguntar al domain expert, etc.
    - Borrar los casos (o columnas) que contienen los `NAs` o reemplazar (imputar) los `NAs` con valores razonables -puedes ocupar modelos para imputar los NA's-
    
- La teoría la veremos más adelante en el curso.

**NOTA**: Todas estás recomendaciones aplican igual para *outliers*

- Es importante recordar que en `R` la operación `x == NA` nunca regresa `TRUE`, siempre hay que utilizar las funciones `is.na()`, `is.nan()` e `is.infinite()`.

- El método `complete.cases` identifica los renglones (individuos) del `data.frame` que no tienen ningún `NA` en sus columnas (variables).

- Es posible usar `sum` y `mean` con `is.na` para obtener el total por columna de faltantes y el porcentaje.
    - ¿Por qué?

Aunque más adelante veremos técnicas más poderosas, vale la pena mencionar 
el ejemplo mostrado en `R in action`, cap. 15. 

La técnica nos permite determinar si los faltantes en una variable están correlacionados con otra.

```{r}
x <- as.data.frame(abs(is.na(algas))) # df es un data.frame

head(x)

# Extrae las variables que tienen algunas celdas con NAs
y <- x[which(sapply(x, sd) > 0)] 

# Da la correación, un valor alto positivo significa que desaparecen juntas.
cor(y) 
```


**Tarea 6. Ejercicio 1**

- Genera un reporte para ambos conjuntos de datos que indique el estado de los valores missing.
- Muestra la matriz de correlación faltante en una gráfica.
- ¿Qué puedes interpretar?

#### **Remover observaciones**

Las variables con más faltantes son Promedio de Cloruro `Cl` (10) y Promedio de Clorofila `Chla` (12).

```{r}
summary(algas[-grep(colnames(algas),pattern = "^a[1-9]")]) # Nota el uso del grep
```

También se puede hacer con `dplyr`

```{r eval=F}
algas %>%
    select(-starts_with("a")) %>%
    summary()
```


![](../images/pointer.png) Antes de removerlas es recomendable verlos, guardarlos y contarlos:

```{r}
nrow(algas[!complete.cases(algas),])
```

Hay `r nrow(algas[!complete.cases(algas),])` observaciones en las cuales tienen `NAs`

```{r}
algas_con_NAs <- algas[!complete.cases(algas),]
```

**Siempre es bueno guardarlas** si se piensan eliminar del dataset, ¿por qué?

Las observaciones con `NAs`son las siguientes (usaremos la función `print()` para explorar)

```{r}
algas_con_NAs[c('max_PH', 'min_O2', 'Cl', 'NO3', 'NH4', 'oPO4', 'PO4', 'Chla')]  %>%
    print(n = 33)
```

de los cuales, hay dos renglones que tienen más del `50%` (`6`) de 
las variables independientes nulas.


Aunque remover las observaciones con `NAs` **NO** sea la estrategia, quitar las observaciones 
con muchas columnas vacías, puede ser recomendable.

En los casos en los que no es posible hacer una explorarción visual, se puede utilizar el siguiente código

```{r}
# ¿Cuántos NAs hay por observación?
apply(algas, 1, function(x) sum(is.na(x)))
```

Si queremos ver las observaciones:

```{r}
algas[apply(algas, 1, function(x) sum(is.na(x))) > 2,]
```

Lo cual confirma nuestra exploración visual.

Si eliminar las observaciones con `NAs` va a ser el camino que vamos a tomar, habrá que hacerlo de manera
reproducible, lo que sigue es el código de la función `indices_con_NAs'

```{r}
indices_con_NAs
```

```{r}
indices_con_NAs(algas, 0.2)
```

```{r}
indices_con_NAs(algas, 0.8)
```

```{r, eval=FALSE}
# Si queremos remover las que tengan más del 20% de NAs...
algas <- algas[-indices_con_NAs(algas, 0.2),]
dim(algas)
```

#### **Renivelar** 

- Si la variable es categórica (`factor`), puedes crear una nueva variable y poner los `NA`s a un nuevo `level`, e.g. `missing`

    - Por ejemplo, suponiendo que hubiese una variable categórica con faltantes en nuestros dataset
  
```{r eval=FALSE}
dataset$cat_with_NAs_fix <- ifelse(is.na(dataset$cat_with_NAs),
                              "missing",
                              ifelse(dataset$ccat_with_NAs == TRUE,    
                                                # o el valor que sea
                                                "level_1",
                                                "level_2"))
```   

- Sólo recuerda que es posible que el valor de `NA` signifique algo.

- Esto también se puede hacer con variables numéricas, si primero las vuelves categóricas (i.e. *binning*)

#### **Centralidad**

- Una estrategia es rellenar los valores faltantes con alguna medida de centralidad.
    - Media, mediana, moda, etc.

- Para variables distribuidas normalmente, esta opción es la mejor.

- Pero para variables *skewed*  o con *outliers* esta decisión puede ser desastrosa.

- Por lo tanto, esta estrategia no se debe de utilizar salvo una exploración previa de las variables.

**Tarea 6. Ejercicio 2**

+ ¿A qué variables del set de datos de `algas` les puedes aplicar este procedimiento?
+ ¿Qué puedes decir de `german_data`?
+ A las variables que no se les puede aplicar, explica por qué no.

- Esta decisión debe de ser reproducible, agrega a `utils.R` una función que impute 
en las variables con  `NAs` el valor central (`median` si es numérica, `moda` si es categórica).
La función debe de tener la siguiente firma:

```{r, eval=FALSE}
imputar_valor_central <- function(data, colnames) {...}
```

#### **Correlación**

Calculando rápidamente la correlación

```{r}
algas[,-c(1:3)] %>%
    cor(use="complete.obs") %>%
    symnum()
```


Observamos que  `oPO4` y `PO4` están altamente relacionadas (`> 0.9`).


```{r correlacion, warning=FALSE, fig.height=4, fig.width=8}
ggplot(data=algas) + 
  aes(x=oPO4, y=PO4) + 
  geom_point(shape=1) + # Usamos una bolita para los puntos
  geom_smooth(method=lm, se=FALSE) +
    theme_hc()
  # Mostramos la linea de la regresión y no mostramos la región de confianza
```

```{r}
algas
```

```{r}
#algas <- algas[-indices_con_NAs(algas, 0.2),]
modelo <- lm(PO4 ~ oPO4, data=algas)
modelo
```

Entonces la fórmula que relaciona el `PO4` con `oPO4` es

$$
PO4 = `r modelo$coefficients['(Intercept)']` + `r modelo$coefficients['oPO4']`*oPO4
$$

**Tarea 6. Ejercicio 3**

+ Crea una función que sustituya los `NAs` con el valor dado por la 
regresión lineal recién calculada (No automatices la regresión lineal) usando la
siguiente firma

```{r, eval=FALSE}
imputar_valor_lm <- function(var_independiente, modelo) { ... }
```

#### **Similitud**

- Podemos suponer que si dos observaciones son similares y una de ellas tiene `NAs` 
en alguna variable, hay una alta probabilidad de que esa variable tenga un valor
similar al valor de esa variable en la otra observación.
    - Obviamente es una suposición...
    
- Debemos definir la noción de similar
    - Y esto significa definir un espacio métrico en el espacio que
       usamos para describir las observaciones.
    - Obviamente, otra gran suposición...

- Para variables numéricas se puede usar la distancia euclídeana

$$
d(\vec{x}, \vec{y}) = \sqrt{\sum_{i=1}^p(\vec{x}_i - \vec{y}_i)}
$$

- Si son nominales las variables

$$
d(\vec{x}, \vec{y}) = \sqrt{\sum_{i=1}^p \delta_i(\vec{x}_i, \vec{y}_i)}
$$

donde $\delta(\vec{x},  \vec{y})$ es la [delta de Kronecker](https://oeis.org/wiki/Kronecker_delta) ¿qué? (;-_-)

La dela de Kronecher identifica si dos variables nominales son iguales o no: 

$$\delta = \begin{cases} 1 & \text{if } x=0 \\ 0 & \text{if } x>0 \end{cases}$$

Ejemplos: 

+ $\delta_{100}^{100}=1$
+ $\delta_4^{134}=0$
+ $\delta_{-1}^{i\pi}=1$
+ $\delta_{7,2^3-1,\frac{21}{3}}=1$
+ $\delta_{-1}^{sin(2017 \sqrt[6]{2})}=0$ 


+ Una vez definida la similitud, debemos de definir el valor que imputar al `NA`.
+ Una opción es utilizar una medida de centralidad de los $k$ observaciones más cercanas. 
+ El Promedio con peso de los valores de los vecinos, es otra opción. El peso se puede determinar de varias
maneras, pero usar como *kernel* una función gaussiana.

$$
peso(d) = e^{-d}
$$

donde $d$ es la distancia de una observación a la que estamos considerando.


- Es importante *estandarizar* los valores numéricos antes de calcular las distancias.

$$
\vec{x}_{normalizado} = \frac{\vec{x}_i - \bar{x}}{\sigma_{x}}
$$

![](../images/pointer.png) ¿Por qué?


**Tarea 6. Ejercicio 4** 

- Implementa una función que impute por similitud con la firma

```{r, eval=FALSE}
imputar_por_similitud <- function(data, num_vecinos) { ... }
```

- Aplícalo a `algas` y `german`. 
- ¿Son muy diferentes las estadísticas ignorando los `NAs` comparadas con este método?


#### **Transformación de datos** 

Paréntesis cultural, en minería de datos: 

- Normalizar $$x_{nuevo}=\frac{x-x_{min}}{x_{max}-x_{min}}$$
- Estandarizar $$x_{nuevo}=\frac{x-\mu}{\sigma}$$

![](../images/pointer.png) Normalizar/estandarizar es útil cuando las cantidades absolutas son menos importantes que las relativas. 


- Normalizar y reescalar 
    - Usar la desviación estándar como unidad de medida.
    - Tiene mucho sentido si la distribución es simétrica.
    - Si no lo es, es posible que sea *lognormally distributed* 
     (como el ingreso monetario o los gastos), una transformación `log10()`
     lo hará útil.
- Es una buena idea usar `log` si el rango de tus datos cubre varios ordenes de magnitud. 
    - Regularmente, estas variables vienen de procesos **multiplicativos** en lugar de aditivos. 

- Si el rango incluye cantidades negativas, usa (crea) una función `signedLog10`

```{r eval=FALSE}
signedLog10 <- function(x) {
  ifelse(abs(x) <= 1.0, sign(x)*log10(abs(x)))
}
```

**Tarea 6, Ejercicio 5**

- Este es un buen momento para dejar de duplicar código y concentrar 
  todas las funciones de `utils.R` que se puedan reutilizar en un archivo `toolset.R`.
  Ajusta tus demás archivos de acuerdo a este cambio -si aplica- 
- Crea un `R notebook` para cada *dataset* utilizando los archivos reproducibles y el archivo
  `toolset.R`. Incluye en estos `notebook` la estructura de los datos, *GEDA* transformaciones de los datos 
  y observaciones pertinentes (como *outliers*, estructura de los datos faltantes). 
  Explica los métodos de imputación que usaste (si fué necesario) y porqué los usaste.


#### EDA $\to$ modelado

- En general la exploración de datos se divide en tres pasos:
    - Verificar la distribución de las variables individuales
        - Identificando *outliers*, valores faltantes $\to$ transformación, eliminación del *dataset*, etc.
    - Verificar la relación entre las variables dependientes y los predictores 
        - Se podrá usar en `feature selection`
    - Relación entre los predictores
        - Eliminación de variables redundantes


### Titanic

#### Datos relacionales y la cuestión de la Semántica

![Titanic](../images/titanic.jpg)

```{r eval=F, message=FALSE, warning=FALSE, include=FALSE}
instalar <- function(paquete) {

    if (!require(paquete,character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)) {
        install.packages(as.character(paquete), dependecies = TRUE, repos = "http://cran.us.r-project.org")
        library(paquete, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)
    }
}

paquetes <- c('lubridate', 'magrittr', 'ggvis', 'dplyr', 'tidyr', 'readr', 'rvest', 
              'ggplot2', 'stringr', 'ggthemes', 'googleVis', 'shiny', 'tibble', 'vcd', 'vcdExtra',
              'GGally', 'readODS', 'readxl', "RSQLite")

lapply(paquetes, instalar)
```

```{r warning=F, error=F, message=F}
titanic_path <- '../data/Titanic/titanic.ods'

ds_names <- ods_sheets(titanic_path)
ds_names
```

Arreglemos los nombres de los *data sets*

```{r}
clean_sheet_name <- function(sheet_name) {
    str_replace_all(str_replace_all(string=sheet_name, pattern=" ", replace="_"), pattern="'", replace="") %>% 
    str_to_lower()
}

sapply(ds_names, clean_sheet_name)
```

Ahora obtenemos los *datasets* y los guardamos

```{r echo=T, warning=F, message=F, error=F}
save_sheet <- function(sheet_name) {
    file_name <-  paste0("../data/Titanic/", clean_sheet_name(sheet_name), ".rds")
    saveRDS(object = read_ods(titanic_path, sheet = sheet_name), file = file_name)
}


lapply(ods_sheets(titanic_path), save_sheet)
```


Cargar los datos: 

```{r}
rm(list=ls())

rds_files <- dir("../data/Titanic/", pattern = "*.rds", full.names = TRUE)

#lapply te devolverá las cosas en un lista... una lista de dataframes :)
ds <- lapply(rds_files, read_rds)
class(ds)
class(ds[[1]])
#cuantos dataframes contiene esta lista? 
length(ds)

#basename elimina todo el path del nombre excepto la última parte (se quedará con la extensión del archivo!), ?basename
names(ds) <- lapply(rds_files, basename)
names(ds)
```

1. En algunos *data sets* se agregaron columnas de más, remuévelas

```{r}
#veamos qué nombres tiene cada dataframe
lapply(ds, names)

#si quisieramos obtener los conjuntos de nombres únicos
lapply(ds, names) %>% unique()
```

Vemos que hay 4 tipos de listas de nombres diferentes: 

+ Las que tienen la últimas columna con un espacio de nombre y la penúltima sin nombre
+ Las que no tienen columnas demás
+ Las que de plano no tienen nombres
+ Las que tienen las últimas dos columnas sin nombre -sutil diferencia con el primer caso- 

Averigüemos más: 

```{r}
lapply(ds, head)
```

El data frame `discharged_crew.rds` tiene dos columnas que no tienen nombre y parecen contener el nombre del empleado, el oficio y la razón de su salida como empleado del barcof. Quitemos este data frame de nuestro set de datos

```{r}
ds <- ds[-which(lapply(lapply(ds, names), length) == 2)]
```

2. Juntemos los data frames en uno solo y quitemos las columnas que están demás

Obtengamos el número mínimo de nombres como base (los demás tienen columnas vacías)
```{r}
num_cols <- lapply((lapply(ds, names)), length) %>% unlist() %>% min()
num_cols 
```

Pero... primero revisemos que los tipos de datos son los mismos -no vaya a ser-
```{r}
lapply(ds, str)
```

(╯°□°)╯︵ ┻━┻   Era demasiado bello para ser real! `Age` y `Body` en algunos *dataframes* son `int` en algunos son `chr`... no los podemos juntar si son de diferentes tipos cambiemos a `chr` todas las columnas por facilidad

```{r}
ds <- lapply(ds, function(x) lapply(x, as.character))
#verifiquemos 
lapply(ds, str)
```


```{r}
#bind_rows es como rbind solo que optimizado por Hadley Wickham :) 
titanic <- bind_rows(ds)[, 1:num_cols]
```

Cambiemos nombres a minúmsculas y sin simbolillos raros `/`
```{r}
names(titanic) <- str_replace_all(names(titanic), "/| ", "_") %>% 
  str_to_lower()
names(titanic)
```

Pasemos el dataframe a un objeto más eficiente
```{r}
titanic <- tbl_df(titanic)
titanic
```


3. Genera las siguientes variables: `survived`, `name`, `last_name`, `sex` [](../images/pointer.png) ¿Se te ocurre alguna forma de definir `survived`?
4. Arregla la columna de precio, edad $\rightarrow$ como verás en aquellos ayeres el dinero no tenía decimales... aquí nos puede ayudar Michael :P [British money](http://projectbritain.com/moneyold.htm). Por otro lado, en la columna edad hay niños con menos de 1 año expresados con el número de meses seguido de `m`...

Haremos el 3 y el 4 juntos :) 

```{r}
titanic <- titanic %>% 
  separate(name, into=c("last_name", "name"), sep=",", extra="drop") %>%
  separate(fare, into=c("pounds", "shillings", "pence"), sep=" ", extra="drop") %>%
  separate(age, into=c("age", "units"), sep=2, extra="drop") %>%
  mutate(sex=ifelse(grepl("Miss|Mrs|Mme.|Lady|Doña|Ms", name), 'F',
                      ifelse(grepl("Mr|Sir|Sig|Dr|Master|Captain|Major|Rev.|Colonel|Fr|Don.", name), 'M', NA))) %>% 
  mutate(boat_location=ifelse(as.integer(boat) %in% c(9:16), 'Popa', 
                              ifelse(boat %in% c(LETTERS[1:4]) | as.integer(boat) %in% c(1:8), 'Proa', NA))) %>% 
  mutate(age=ifelse(units == "m", 1, as.integer(age))) %>% 
  mutate(survived=!is.na(boat)) %>%
  dplyr::select(-c(shillings, pence, body, units)) %>%
  mutate(pounds=str_replace(pounds, "£", "") %>% as.integer()) %>%
  mutate(class_dept=as.factor(class_dept), group=as.factor(group), ship=as.factor(ship),
         joined=as.factor(joined), job=as.factor(job), boat=as.factor(boat),
         sex=as.factor(sex), boat_location=as.factor(boat_location))
  
```

Que bonito es `dplyr` y `tidyr` (ﾉ^_^)ﾉ

```{r}
summary(titanic)
```

5. Agrega una columna de `age` que sea categórica. Definamos 3 categorías: 

+ **infante:** si es menor de 18 años
+ **adulto:** entre 18 y 65 años
+ **adulto mayor:** si es mayor a 65 años

```{r}
titanic <- titanic %>% mutate(age = ifelse(age <= 18, "infante",
                                           ifelse(age > 65, "adulto mayor", 
                                                  "adulto")))
#verifiquemos
titanic
```


6. Ajusta a precios del día de hoy (Por ejemplo usa esta [página](http://inflation.stephenmorley.org/)) ¿En que clase hubieras viajado? ¿Cuál era tu probilidad de supervivencia?

```{r}
ggplot(titanic, aes(pounds)) + 
  geom_histogram(binwidth = 30, na.rm=T) +
  theme_bw()


titanic <- titanic %>% 
    group_by(ticket) %>% 
    mutate(pounds_per_ticket = round(pounds/n())) %>% 
  ungroup()

titanic

titanic %>% filter(class_dept %in% c('1st Class', '2nd Class', '3rd Class')) %>%
  ggplot(aes(pounds_per_ticket)) + 
  geom_histogram(binwidth = 10) + 
  facet_grid(class_dept~., scales = "free_y") +
  theme_bw()
```

$\rightarrow$ Aproximadamente 10 libras de 1912 son 1,080 libras actuales, 50 libras son 5,400 y 100 libras son 10,800 libras al 2017

7. Observando la distribución de botes que se muestra en la figura ¿Qué puedes decir sobre como se utilizaron?
  ¿Coincide con la película de Titanic de James Cameron?

![](../images/deck_plan_titanic.jpg)

```{r}
titanic %>% 
    group_by(boat_location) %>% 
    summarise(n=n())
```

Eso no dice mucho...

```{r}
titanic %>%
    group_by(boat) %>%
    summarise(n=n()) %>%
    arrange(desc(n))
```

Los botes del 1 al 16 tenían una capacidad de 65 personas, los botes del **A** al **D** tenían una capacidad de 45 personas. ([Fuente](https://en.wikipedia.org/wiki/RMS_Titanic)).

Fuente de los datos: 

[Encyclopedia Titanica](https://www.encyclopedia-titanica.org/)

### Berka

```{r eval=F}
instalar <- function(paquete) {

    if (!require(paquete,character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)) {
        install.packages(as.character(paquete), dependecies = TRUE, repos = "http://cran.us.r-project.org")
        library(paquete, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)
    }
}

paquetes <- c('lubridate', 'magrittr', 'ggvis', 'dplyr', 'tidyr', 'readr', 'rvest', 
              'ggplot2', 'stringr', 'ggthemes', 'googleVis', 'shiny', 'tibble', 'vcd', 'vcdExtra',
              'GGally', 'readODS', 'readxl', "RSQLite")

lapply(paquetes, instalar);
```

Los datos están en `http://sorry.vse.cz/~berka/challenge/pkdd1999/data_berka.zip`, descárgalos y 
guárdalos en `data/berka`, fueron usados en la competencia de [PKDD de 1999](http://lisp.vse.cz/pkdd99/). 

#### Meta

"El banco busca mejorar sus servicios, Por ejemplo, los admnistradores del banco 
tienen únicamente una vaga idea de quién es un buen cliente (al cual ofrecerle
más servicios) y quién es un mal cliente (alguien al que hay que cuidar para 
minimizar las pérdidas del banco). Afortunadamente, el banco almacena datos 
de sus clientes, sus cuentas (transacciones), préstamos que ya se otorgaron, 
tarjetas de crédito dadas. Los administradaores del banco esperan mejorar
su entendimiento de los clientes y mejorar así sus servicios. Una mera aplicación
de una herramienta de *discovery* no los va a convencer."


#### Esquema

##### Account

Características de la cuenta
Archivo: `account`

 Column|Description|Notes
--------|------------|-----------------------|
account_id	|Identification of the account|
district_id	|Location of the branch	 |
date	|Date of the account's creation	|In the form: YYMMDD 
frequency	|Frequency of statement issuance|	"POPLATEK MESICNE" - Monthly Issuance
| | |"POPLATEK TYDNE" - Weekly Issuance
| | | "POPLATEK PO OBRATU" - Issuance After Transaction


##### Client

Características del cliente
Archivo: `client`

Column	|Description|	Notes
--------|------------|---------------------------|
client_id	|Client Identifier	 |
birth number	|Birthday and Sex |	The value is in the form: YYMMDD (for men)
| | | The value is in the form: YYMM+50DD (for women)
| | |Where YYMMDD is the date of birth
district_id	|Address of the client	 |

	

##### Disposition 

Relaciona una cuenta con un cliente (los derechos  de los clientes para
operar cuentas)
Archivo: `disp`

Column	|Description	|Notes
--------|------------|-----------------------------------|
disp_id	|Record Identifier	 |
client_id	|Client Identifier|	 
account_id	|Account Identifier	 |
type	| Type of Disposition (owner/user)	|Only owner can issue permanent orders and ask for a loan

##### Loan

Préstamos dabos a una cuenta
Archivo: `loan`

Column	|Description	|Notes
--------|------------|-----------------------------------|
disp_id	|Record Identifier	 |
loan_id	|Record Identifier	 |
account_id	|Account Identifier	 |
date	|Date when loan was granted	|In the form: YYMMDD
amount	|Amount of Loan	 |
duration	|Duration of Loan	 |
payments	|Monthly Payments on Loan	 |
status	|Status in paying off the loan	|'A' stands for contract finished, no problems
| | | 'B' stands for contract finished, loan not payed
| | | 'C' stands for running contract, OK thus-far
| | | 'D' stands for running contract, client in debt

##### Order

Características de una orden de pago
Archivo: `order`

Column	|Description	|Notes
--------|------------|--------------------------------|
order_id	|Record Identifier	 |
account_id	|Account the order is issued for	 |
bank_to	|Bank of the recipient	|Each bank has a unique two-letter code
account_to|	Account of the recipient	 |
amount	|Amount debited from order account |	 
K_symbol|	Characterization of the payment|	'POJISTNE' stands for Insurance Payment
| | |'SIPO' stands for Household Payment
| | |'LEASING' stands for Leasing Payment
| | | 'UVER' stands for Loan Payment

##### Transaction

Transacciones en las cuentas
Archivo: `trans`

Column	|Description	|Notes
--------|------------|---------------------------|
trans_id |	Record Identifier	 |
account_id	|Account the transaction is issued on	 |
date	|Date of transaction	|In the form: YYMMDD
type	|debit/credit transaction	|'PRIJEM' stands for Credit
| | | 'VYDAJ' stands for Debit (withdrawal)
operation	|Mode of Transaction	|'VYBER KARTOU' stands for Credit Card Withdrawal
| | |'VKLAD' stands for Credit in Cash
| | |'PREVOD Z UCTU' stands for Collection from Another Bank
| | |'VYBER' stands for Withdrawal in Cash
| | | 'PREVOD NA UCET' stands for Remittance to Another Bank
amount	|Amount of Transaction	 |
balance	|Balance of Account after Transaction	 |
K_Symbol	|Characterization of Transaction	|'POJISTNE' stands for Insurance Payment
| | |'SLUZBY' stands for Payment of Statement
| | |'UROK' stands for Interest Credited
| | |'SANKC. UROK' stands for Sanction Interest if Negative Balance
| | |'SIPO' stands for Household Payment
| | |'DUCHOD' stands for Old-age Pension Payment
| | |'UVER' stands for Loan Payment
bank	|Bank of the partner	|Each bank has unique two-letter code
account	|Account of the partner	 |

##### Demographic

Características  de un distrito
Archivo: `district`


Column	|Description	|Notes
--------|------------|----------------------|
A1 | district_id|	District Identifier	 
A2	|District Name	 
A3	|Region	 
A4	|No. of Inhabitants	 
A5	|No. of Municipalities with inhabitants < 499	 
A6	|No. of Municipalities with inhabitants 500-1999	 
A7	|No. of Municipalities with inhabitants 2000-9999	 
A8	|No. of Municipalities with inhabitants > 10000	 
A9	|No. of Cities	 
A10	|Ratio of urban inhabitants	 
A11	|Average Salary	 
A12	|Unemployment rate in 1995	 
A13	|Unemployment rate in 1996	 
A14	|No. of Enterpreneurs per 1000 inhabitants	 
A15	|No. of Crimes commited in 1995	 
A16	|No. of Crimes commited in 1996

##### Credit card

Tarjeta de crédito otorgada a una cuenta
Archivo: `card` 

Column	|Description	|Notes
--------|------------|--------------------------|
card_id	|Card Identifier	 |
disp_id	|Disposition to an account	 |
type	|Type of card	|Types are 'Junior', 'Classic', and 'Gold'
issued	|Date card was issued	|In the format: YYMMDD

#### Características importantes

- Cada cuenta tiene características estáticas (`account`) y dinámicas
(pagos, créditos, balances) dados en `order` y `transaction`
- `client` características de personas que pueden manipular cuentas. 
- Los clientes pueden tener varias cuentas, y viceversa. La relación entre clientes y cuentas está en `disposition`
- Los servicios del banco están descritos en `loan` y `credit_card`
- Una cuenta puede tener varias **tdc**
- Máximo un préstamo se le puede otorgar a una cuenta.
- La tabla de demografía contiene informacióń sobre los distritos, se puede
deducir información adicional sobre los clientes a partir de esta tabla.

#### Esquema Entidad-Relación

![Berka dataset](http://lisp.vse.cz/pkdd99/Challenge/data.gif)

#### Base de datos

La utilización de una base de datos (Relacional, Grafos, Columnar, etc), siempre es recomendable sobre 
el uso de archivos (la excepción es un Apache Hadoop, pero eso lo veremos en Métodos de Gran Escala ╭(◔ ◡ ◔)/).

Las ventajas de la utilización de una base de datos son las siguientes:

- Es posible manejar volúmenes de datos más grandes que memoria.
- Velocidad de acceso a los datos
- Facilidad de manipulación con lenguajes relacionales (como `SQL`)
- Colaboración
    - Varias personas pueden accesar a los datos
    - Estas personas no tienen por qué usar el mismo lenguaje (!)

Usaremos `sqlite3`  para este ejemplo. `SQLite` es una base de datos relacional, local
(por lo que algunas de las ventajas recién mencionadas no aplican ¿Cuál?)

+ Instalar SQLite3 en Ubuntu 16.04: 

```{shell eval=FALSE}
sudo apt-get update
sudo apt-get install sqlite3
```

+ Instalar SQLite3 en MAC: si tienes OS Leopard en adelante, no necistas instalarlo, ya viene. Si tienes un OS más viejito [Mac y Windows](https://mislav.net/rails/install-sqlite3/) -nunca lo he hecho!-

+ [SQLite3 CLI](https://sqlite.org/cli.html)

*NOTA: Lo que sigue ocurre en la línea de comandos*

Para abrir una base de datos `sqlite` en memoria

```{shell eval=FALSE}
$ sqlite3
SQLite version 3.13.0 2016-05-18 10:57:30
Enter ".help" for usage hints.
Connected to a transient in-memory database.
Use ".open FILENAME" to reopen on a persistent database.
sqlite> 
```

```{shell eval=FALSE}
## Emebelliciendo el output
sqlite> .header on
sqlite> .mode column


## Importar archivos (accounts.asc) a la base de datos a la tabla (accounts)
## ¡Usa plural para las tablas!
sqlite> .separator ";"
sqlite> .import account.asc accounts

## Verifica que la tabla exista
sqlite> .tables
acccounts

## Estructura de la tabla
sqrlite> PRAGMA table_info(accounts);
cid         name        type        notnull     dflt_value  pk        
----------  ----------  ----------  ----------  ----------  ----------
0           account_id  TEXT        0                       0         
1           district_id TEXT        0                       0         
2           frequency   TEXT        0                       0         
3           date        TEXT        0                       0    

## veamos qué hay adentro
sqrlite> select * from accounts limit 5;
account_id  district_id  frequency         date      
----------  -----------  ----------------  ----------
576         55           POPLATEK MESICNE  930101    
3818        74           POPLATEK MESICNE  930101    
704         55           POPLATEK MESICNE  930101    
2378        16           POPLATEK MESICNE  930101    
2632        24           POPLATEK MESICNE  930102  

## Guardar la tabla
sqlite> .save berka.raw
```

La siguiente vez que quieras consultar la base de datos

```{shell eval=FALSE}
$ sqlite3 berka.raw
SQLite version 3.11.0 2016-02-15 17:29:24
Enter ".help" for usage hints.
sqlite> 
```

O si se te olvida poner el nombre del archivo:

```{shell eval=FALSE}
$ sqlite3
SQLite version 3.11.0 2016-02-15 17:29:24
Enter ".help" for usage hints.
Connected to a transient in-memory database.
Use ".open FILENAME" to reopen on a persistent database.
sqlite> .open berka.raw
```

**Ejercicio: To Raw**

Cargaremos los datos en `berka.raw`

```{shell eval=FALSE}
for db in *asc;
do
table=${db%.*}s
if [ "$db" = "trans.asc" ]; then
table="transactions"
fi
echo -e ".separator ';'\n.import ${db} ${table}" | sqlite3 berka.raw 
done
```

Puedes conectarte a tu base de sqlite con [dplyr](http://dplyr.tidyverse.org/reference/src_dbi.html) , necesitarás el paquete `RSQLite`

```{r}
library(RSQLite)

berka_db <- src_sqlite(path="~/Documents/itam/introduction_to_ds/intro_to_ds/data/berka/berka.raw", 
           create=FALSE)
db_list_tables(berka_db$con)

accounts_tbl <- tbl(berka_db, "accounts")
clients_tbl <- tbl(berka_db, "clients")
dispositions_tbl <- tbl(berka_db, "disps")

accounts_tbl %>% group_by(district_id) %>%
  summarise(count=n()) %>% 
  arrange(desc(count)) %>%
  collect()
```

**Ejercicio: Raw to Clean**

1. Verifica que cada `account` tenga un `owner`

En la tabla disposition viene el atributo `type` que puede tener 2 valores distintos: `OWNER` y `DISPONENT`

```{shell eval=FALSE}
sqlite3> select distinct(type) 
from disps;
```

O en dplyr: 
```{r}
dispositions_tbl %>% distinct(type) %>% collect()
```


```{shell eval=FALSE}
sqlite3> select *, count(*)
from disps
where type = 'ONWER'
group by account_id
having count(*) > 1
order by count(*) desc
limit 5;
```

```{r}
dispositions_tbl %>% filter(type == 'OWNER') %>% 
  group_by(account_id) %>% 
  summarise(n=n()) %>%
  arrange(desc(n)) %>% collect()
```

- Los registros de  `orders` y `loans` están duplicados en `transactions`. Es decir
los regsitros de `order`y `loan` están dentro de  `transactions` (por ejemplo, 
Los registros de `loan` en `tran` están identificados por el `k_symbol` `LP`)


- Traduce los campos del checo al inglés.
- En `client` cambia `BirthNumber` a `sex` y `age`
- Discretiza `age` en `Youth` (0-24), `Adult` (25-45), `Middle-age` (46-64) y `Senior` (> 65)
- En `disposition` cambia de `Dispondent` -> `User` 
- En `loan` discretiza usando alguna heuristica `amount`, `duration`  y `payments`
- En `transaction` traduce la columna `type`, `operation`, `k_symbol`
- Ajustamos los nombres de las tablas a plural, los `*_id` a singular.
- Guardemos los datos en `berka.clean`

### Limpieza retos técnicos

Imágenes tomadas de [TAMR](https://www.tamr.com/)

![](../images/tamr_a.png)

<br>

![](../images/tamr_b.png)

<br>

![](../images/tamr_c.png)

<br>

![](../images/tamr_d.png)

<br>

![](../images/tamr_e.png)

<br>

![](../images/tamr_f.png)


#### Primer acercamiento a **Linage and Procedence**: Columnas de procedencia -te recomiendo leer este [blog](https://blog.datank.ai/data-lineage-the-history-of-your-data-ebe1c143d608)-


Un aspecto que siempre es olvidado, o que no se considera importante debido a los blogs, es el versionado de control de los datos

Esto se puede implementar, agregando columnas para indicar de donde vienen los datos, o con qué procedimiento de limpieza se generaron, etc.

Se puede utilizar el mismo id del código del ETL guardado en github.
¿Cómo implementarías esto? ¿Puedes trazar de dónde provienen tus datos?